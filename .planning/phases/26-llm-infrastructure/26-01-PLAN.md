---
phase: 26-llm-infrastructure
plan: 01
type: execute
depends_on: ["25-01"]
files_modified: [src/reddit_insight/llm/client.py, src/reddit_insight/llm/prompts.py, src/reddit_insight/config.py]
---

<objective>
LLM API 통합 인프라를 구축한다: Claude/OpenAI API 클라이언트, 프롬프트 템플릿, rate limiting.

Purpose: LLM 기반 고급 분석 기능의 기반 마련
Output: LLMClient, PromptTemplate, RateLimiter
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/reddit_insight/config.py

**LLM 통합 요구사항:**
- Claude API (Anthropic) 우선
- OpenAI API 백업 지원
- API 비용 관리 (rate limiting, caching)
- 프롬프트 버전 관리
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement LLM Client</name>
  <files>src/reddit_insight/llm/__init__.py, src/reddit_insight/llm/client.py, pyproject.toml</files>
  <action>LLM API 클라이언트를 구현한다:

  1. 의존성 추가: anthropic, openai

  2. LLMClient 추상 클래스:
     ```python
     class LLMClient(ABC):
         @abstractmethod
         async def complete(self, prompt: str, **kwargs) -> str: ...
         @abstractmethod
         async def complete_with_retry(self, prompt: str, max_retries: int = 3) -> str: ...
     ```

  3. ClaudeClient 구현:
     ```python
     class ClaudeClient(LLMClient):
         def __init__(self, api_key: str, model: str = "claude-3-haiku-20240307"):
             self.client = anthropic.Anthropic(api_key=api_key)
             self.model = model

         async def complete(self, prompt: str, max_tokens: int = 1024) -> str:
             message = self.client.messages.create(
                 model=self.model,
                 max_tokens=max_tokens,
                 messages=[{"role": "user", "content": prompt}]
             )
             return message.content[0].text
     ```

  4. OpenAIClient 백업 구현

  5. 설정 추가 (config.py):
     - LLM_PROVIDER: "claude" | "openai"
     - ANTHROPIC_API_KEY
     - OPENAI_API_KEY
     - LLM_MODEL</action>
  <verify>pytest tests/llm/test_client.py -v (mock 사용)</verify>
  <done>LLMClient 구현 완료</done>
</task>

<task type="auto">
  <name>Task 2: Create Prompt Templates</name>
  <files>src/reddit_insight/llm/prompts.py, src/reddit_insight/llm/templates/</files>
  <action>프롬프트 템플릿 시스템을 구현한다:

  1. PromptTemplate 클래스:
     ```python
     class PromptTemplate:
         def __init__(self, template: str, version: str = "1.0"):
             self.template = template
             self.version = version

         def format(self, **kwargs) -> str:
             return self.template.format(**kwargs)
     ```

  2. 분석용 프롬프트 템플릿:
     - SUMMARIZE_POSTS: 게시물 요약
     - CATEGORIZE_CONTENT: 콘텐츠 카테고리 분류
     - EXTRACT_INSIGHTS: 인사이트 추출
     - SENTIMENT_ANALYSIS: 심층 감성 분석
     - TREND_INTERPRETATION: 트렌드 해석

  3. 프롬프트 관리:
     - 버전 관리
     - A/B 테스트 지원
     - 토큰 수 추정</action>
  <verify>프롬프트 템플릿 로딩 및 포맷팅 테스트</verify>
  <done>프롬프트 템플릿 시스템 완성</done>
</task>

<task type="auto">
  <name>Task 3: Implement Rate Limiting and Caching</name>
  <files>src/reddit_insight/llm/rate_limiter.py, src/reddit_insight/llm/cache.py</files>
  <action>API 호출 제한 및 캐싱을 구현한다:

  1. RateLimiter 클래스:
     ```python
     class RateLimiter:
         def __init__(self, requests_per_minute: int = 60, tokens_per_minute: int = 100000):
             self.rpm_limit = requests_per_minute
             self.tpm_limit = tokens_per_minute
             self._request_times: list[float] = []
             self._token_counts: list[tuple[float, int]] = []

         async def acquire(self, estimated_tokens: int = 0) -> None:
             """Rate limit 확인 후 대기"""

         def estimate_tokens(self, text: str) -> int:
             """텍스트의 토큰 수 추정 (약 4글자 = 1토큰)"""
     ```

  2. LLMCache 클래스:
     - 프롬프트 해시 기반 캐싱
     - TTL 설정 (기본 24시간)
     - 캐시 히트율 통계

  3. LLMClient에 rate limiter와 cache 통합:
     ```python
     class ClaudeClient(LLMClient):
         def __init__(self, ..., rate_limiter: RateLimiter, cache: LLMCache):
             ...

         async def complete(self, prompt: str, use_cache: bool = True) -> str:
             if use_cache:
                 cached = self.cache.get(prompt)
                 if cached:
                     return cached

             await self.rate_limiter.acquire(self.rate_limiter.estimate_tokens(prompt))
             result = await self._call_api(prompt)

             if use_cache:
                 self.cache.set(prompt, result)
             return result
     ```</action>
  <verify>rate limiter 동작 테스트, 캐시 히트/미스 테스트</verify>
  <done>Rate limiting, caching 구현 완료</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] LLMClient mock 테스트 통과
- [ ] 프롬프트 템플릿 로딩 테스트
- [ ] Rate limiter 단위 테스트
- [ ] 캐시 단위 테스트
- [ ] 환경 변수 설정 문서화
</verification>

<success_criteria>

- ClaudeClient, OpenAIClient 구현
- 5개 이상의 프롬프트 템플릿
- Rate limiting (RPM, TPM)
- 프롬프트 응답 캐싱
</success_criteria>

<output>
After completion, create `.planning/phases/26-llm-infrastructure/26-01-SUMMARY.md`
</output>
