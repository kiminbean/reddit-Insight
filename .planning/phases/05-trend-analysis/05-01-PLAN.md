---
phase: 05-trend-analysis
plan: 01
type: execute
depends_on: ["04-02"]
files_modified: [src/reddit_insight/analysis/__init__.py, src/reddit_insight/analysis/tokenizer.py, src/reddit_insight/analysis/stopwords.py, pyproject.toml]
---

<objective>
텍스트 전처리 및 토큰화 시스템을 구축한다.

Purpose: Reddit 텍스트를 분석 가능한 토큰으로 변환
Output: Tokenizer, 불용어 처리, n-gram 추출
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Research findings:**
- YAKE: 가볍고 학습 불필요, 통계 기반 키워드 추출
- TF-IDF: scikit-learn으로 구현, 코퍼스 기반
- Reddit 특성: 슬랭, 이모지, 축약어 많음

**Prior context:**
- TextPreprocessor가 pipeline/preprocessor.py에 존재 (기본 정제)
- Post, Comment 데이터 수집 및 저장 가능

**Constraints:**
- 무료/오픈소스 도구만 사용
- 로컬 실행 (외부 API 없음)
- 다국어 지원 (영어 우선, 확장 가능)
</context>

<tasks>

<task type="auto">
  <name>Task 1: 의존성 추가 및 analysis 모듈 구조</name>
  <files>pyproject.toml, src/reddit_insight/analysis/__init__.py</files>
  <action>
  1. pyproject.toml에 NLP 의존성 추가:
     - "yake>=0.4.8" (키워드 추출)
     - "scikit-learn>=1.3.0" (TF-IDF, 기존에 없으면)
     - "nltk>=3.8.0" (토큰화, 불용어)

  2. src/reddit_insight/analysis/ 디렉토리 생성

  3. src/reddit_insight/analysis/__init__.py 생성:
     - 모듈 레벨 export 정의
     - __all__ 준비
  </action>
  <verify>
  - cat pyproject.toml | grep -E "yake|scikit-learn|nltk"
  - ls -la src/reddit_insight/analysis/
  </verify>
  <done>
  - NLP 의존성 추가됨
  - analysis 모듈 구조 생성됨
  </done>
</task>

<task type="auto">
  <name>Task 2: 불용어 관리 모듈</name>
  <files>src/reddit_insight/analysis/stopwords.py</files>
  <action>
  src/reddit_insight/analysis/stopwords.py 생성:

  1. StopwordManager 클래스:
     - __init__(language: str = "english")
     - _base_stopwords: set[str] (NLTK에서 로드)
     - _custom_stopwords: set[str]
     - _reddit_stopwords: set[str] (Reddit 특화)

  2. Reddit 특화 불용어:
     - "reddit", "subreddit", "post", "comment"
     - "upvote", "downvote", "karma"
     - "edit", "deleted", "removed"
     - "http", "https", "www", "com"
     - 일반적인 Reddit 표현들

  3. 메서드:
     - get_stopwords() -> set[str]: 전체 불용어 반환
     - add_stopwords(words: Iterable[str]): 불용어 추가
     - remove_stopwords(words: Iterable[str]): 불용어 제거
     - is_stopword(word: str) -> bool

  4. 유틸리티 함수:
     - get_default_stopwords(language: str = "english") -> set[str]
     - ensure_nltk_data(): NLTK 데이터 다운로드 (필요시)
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.analysis.stopwords import StopwordManager; sm = StopwordManager(); print(len(sm.get_stopwords()), 'stopwords')"
  </verify>
  <done>
  - 불용어 관리 시스템 구현
  - Reddit 특화 불용어 포함
  </done>
</task>

<task type="auto">
  <name>Task 3: 토크나이저 구현</name>
  <files>src/reddit_insight/analysis/tokenizer.py</files>
  <action>
  src/reddit_insight/analysis/tokenizer.py 생성:

  1. TokenizerConfig 데이터 클래스:
     - lowercase: bool = True
     - remove_stopwords: bool = True
     - min_token_length: int = 2
     - max_token_length: int = 50
     - remove_numbers: bool = False
     - remove_punctuation: bool = True
     - language: str = "english"

  2. RedditTokenizer 클래스:
     - __init__(config: TokenizerConfig | None = None)
     - _config: TokenizerConfig
     - _stopword_manager: StopwordManager
     - _word_pattern: re.Pattern

  3. 토큰화 메서드:
     - tokenize(text: str) -> list[str]:
       - 단어 토큰화
       - 설정에 따라 필터링
     - tokenize_batch(texts: list[str]) -> list[list[str]]

  4. N-gram 추출:
     - get_ngrams(tokens: list[str], n: int = 2) -> list[str]:
       - bigram, trigram 등 추출
     - get_ngrams_from_text(text: str, n: int = 2) -> list[str]

  5. 토큰 정규화:
     - _normalize_token(token: str) -> str | None:
       - 길이 검사, 불용어 필터링
       - None 반환 시 제외

  주의:
  - Reddit 특화 처리: 이모지, URL 잔여물 제거
  - 숫자 포함 토큰은 선택적 유지 (버전 번호 등)
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.analysis.tokenizer import RedditTokenizer; t = RedditTokenizer(); print(t.tokenize('Hello World! This is a test post.'))"
  </verify>
  <done>
  - RedditTokenizer가 텍스트 토큰화
  - N-gram 추출 지원
  </done>
</task>

<task type="auto">
  <name>Task 4: export 및 통합 테스트</name>
  <files>src/reddit_insight/analysis/__init__.py</files>
  <action>
  1. __init__.py 업데이트:
     - RedditTokenizer, TokenizerConfig export
     - StopwordManager, get_default_stopwords export
     - __all__ 정의

  2. 통합 확인:
     - 토큰화 파이프라인 테스트
     - 불용어 필터링 테스트
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.analysis import RedditTokenizer, StopwordManager; print('All exports OK')"
  </verify>
  <done>
  - 토큰화 시스템 완성
  - 모듈 export 완료
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] yake, scikit-learn, nltk 의존성 추가됨
- [ ] StopwordManager가 Reddit 특화 불용어 포함
- [ ] RedditTokenizer가 텍스트 토큰화 및 n-gram 추출
- [ ] 모듈 import 정상 동작
</verification>

<success_criteria>

- 모든 태스크 완료
- Reddit 텍스트 토큰화 가능
- 불용어 필터링 동작

</success_criteria>

<output>
After completion, create `.planning/phases/05-trend-analysis/05-01-SUMMARY.md`
</output>
