---
phase: 03-web-scraping
plan: 01
type: execute
depends_on: ["02-01"]
files_modified: [src/reddit_insight/scraping/__init__.py, src/reddit_insight/scraping/http_client.py, src/reddit_insight/scraping/rate_limiter.py, pyproject.toml]
---

<objective>
웹 스크래핑 인프라를 설정한다.

Purpose: API 제한 시 백업으로 사용할 스크래핑 기반 구축
Output: HTTP 클라이언트, Rate Limiter, User-Agent 로테이션
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-reddit-api/02-01-SUMMARY.md

**Research findings:**
- old.reddit.com은 JS 불필요, 전통적 스크래핑에 적합
- URL에 .json 추가하면 JSON 응답 (비공식 API)
- httpx로 async HTTP 요청
- Rate limiting: rotating User-Agent, 요청 간격 조절 필요
- Reddit은 스크래퍼 차단하므로 프록시/헤더 로테이션 권장

**Prior phase context:**
- httpx가 이미 의존성에 포함됨 (pyproject.toml)
- Post, Comment 모델이 정의됨 (재사용)

**Constraints:**
- 무료 도구만 사용 (유료 프록시 서비스 제외)
- 윤리적 스크래핑 (rate limit 준수)
</context>

<tasks>

<task type="auto">
  <name>Task 1: 스크래핑 모듈 구조 및 의존성</name>
  <files>pyproject.toml, src/reddit_insight/scraping/__init__.py</files>
  <action>
  1. pyproject.toml에 스크래핑 의존성 추가:
     - "beautifulsoup4>=4.12.0" (HTML 파싱)
     - "lxml>=5.0.0" (빠른 파서)

  2. src/reddit_insight/scraping/ 디렉토리 생성

  3. src/reddit_insight/scraping/__init__.py 생성:
     - 모듈 레벨 export 정의
     - __all__ = ["ScrapingClient", "RateLimiter", "RedditScraper"]
  </action>
  <verify>
  - cat pyproject.toml | grep -E "beautifulsoup|lxml"
  - python -c "import sys; sys.path.insert(0, 'src')"
  </verify>
  <done>
  - beautifulsoup4, lxml 의존성 추가됨
  - scraping 모듈 디렉토리 구조 생성됨
  </done>
</task>

<task type="auto">
  <name>Task 2: Rate Limiter 구현</name>
  <files>src/reddit_insight/scraping/rate_limiter.py</files>
  <action>
  src/reddit_insight/scraping/rate_limiter.py 생성:

  1. RateLimiter 클래스:
     - __init__(requests_per_minute: int = 30, min_delay: float = 1.0)
     - async wait() 메서드: 다음 요청까지 대기
     - _last_request_time: float
     - _request_count: int
     - _window_start: float

  2. 기능:
     - 슬라이딩 윈도우 기반 rate limiting
     - 분당 요청 수 제한
     - 최소 요청 간격 보장

  3. 컨텍스트 매니저 지원:
     - async with rate_limiter: 자동 대기

  주의:
  - Reddit 스크래핑은 보수적으로 30 req/min 기본값
  - API(60 req/min)보다 느리게 설정
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping.rate_limiter import RateLimiter; print('RateLimiter OK')"
  </verify>
  <done>
  - RateLimiter가 요청 속도 제어
  - async/await 지원
  </done>
</task>

<task type="auto">
  <name>Task 3: HTTP 클라이언트 구현</name>
  <files>src/reddit_insight/scraping/http_client.py</files>
  <action>
  src/reddit_insight/scraping/http_client.py 생성:

  1. USER_AGENTS 상수:
     - 10개 이상의 일반적인 브라우저 User-Agent 문자열

  2. ScrapingClient 클래스:
     - __init__(rate_limiter: RateLimiter | None = None)
     - _client: httpx.AsyncClient (lazy init)
     - _user_agents: list[str]
     - _current_ua_index: int

  3. 메서드:
     - async get(url: str, params: dict | None = None) -> httpx.Response
       - User-Agent 로테이션
       - rate_limiter.wait() 호출
       - 재시도 로직 (3회, exponential backoff)
     - async get_json(url: str) -> dict
       - .json URL 요청 및 파싱
     - _rotate_user_agent() -> str
     - async close()

  4. 컨텍스트 매니저:
     - async with ScrapingClient() as client: ...

  주의:
  - httpx.AsyncClient 사용 (이미 의존성에 있음)
  - 429 응답 시 백오프
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping.http_client import ScrapingClient; print('ScrapingClient OK')"
  </verify>
  <done>
  - ScrapingClient가 User-Agent 로테이션 지원
  - Rate limiting과 재시도 로직 구현
  </done>
</task>

<task type="auto">
  <name>Task 4: __init__.py 업데이트 및 테스트</name>
  <files>src/reddit_insight/scraping/__init__.py</files>
  <action>
  1. __init__.py 업데이트:
     - ScrapingClient, RateLimiter export
     - __all__ 정의

  2. 간단한 통합 확인:
     - import가 정상 동작하는지 확인
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping import ScrapingClient, RateLimiter; print('All exports OK')"
  </verify>
  <done>
  - 스크래핑 인프라 기본 구성 완료
  - 모든 export 정상 동작
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] beautifulsoup4, lxml 의존성 추가됨
- [ ] RateLimiter가 요청 속도 제어
- [ ] ScrapingClient가 User-Agent 로테이션 및 재시도 지원
- [ ] from reddit_insight.scraping import 가능
</verification>

<success_criteria>

- 모든 태스크 완료
- 스크래핑 인프라 기반 구축
- Rate limiting 및 anti-blocking 대책 구현

</success_criteria>

<output>
After completion, create `.planning/phases/03-web-scraping/03-01-SUMMARY.md`
</output>
