---
phase: 03-web-scraping
plan: 03
type: execute
depends_on: ["02-01", "03-02"]
files_modified: [src/reddit_insight/data_source.py, src/reddit_insight/__init__.py, tests/test_data_source.py]
---

<objective>
API와 스크래핑 간 자동 전환 로직을 구현한다.

Purpose: API 실패/제한 시 자동으로 스크래핑으로 폴백
Output: UnifiedDataSource 클래스 (API/스크래핑 투명한 전환)
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-reddit-api/02-01-SUMMARY.md
@.planning/phases/03-web-scraping/03-02-PLAN.md

**Prior context:**
- RedditClient (PRAW 기반 API)
- RedditScraper (스크래핑 기반)
- 동일한 데이터 모델 사용 (Post, Comment, SubredditInfo)

**Fallback strategy:**
1. API 우선 시도
2. API 실패 시 (rate limit, 인증 실패 등) 스크래핑으로 전환
3. 스크래핑도 실패 시 에러 발생
4. 성공한 소스 기억하여 다음 요청에 활용
</context>

<tasks>

<task type="auto">
  <name>Task 1: DataSourceStrategy 열거형 및 예외 정의</name>
  <files>src/reddit_insight/data_source.py</files>
  <action>
  src/reddit_insight/data_source.py 생성:

  1. DataSourceStrategy 열거형:
     - API_ONLY: API만 사용
     - SCRAPING_ONLY: 스크래핑만 사용
     - API_FIRST: API 우선, 실패 시 스크래핑 (기본값)
     - SCRAPING_FIRST: 스크래핑 우선, 실패 시 API

  2. 예외 클래스:
     - DataSourceError(Exception): 모든 소스 실패
     - APIUnavailableError(DataSourceError): API 사용 불가
     - ScrapingUnavailableError(DataSourceError): 스크래핑 사용 불가

  3. SourceStatus 데이터 클래스:
     - api_available: bool
     - scraping_available: bool
     - last_api_error: str | None
     - last_scraping_error: str | None
     - api_failure_count: int
     - scraping_failure_count: int
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.data_source import DataSourceStrategy, DataSourceError; print('Enums OK')"
  </verify>
  <done>
  - 전략 열거형 및 예외 정의됨
  - SourceStatus로 상태 추적
  </done>
</task>

<task type="auto">
  <name>Task 2: UnifiedDataSource 기본 구현</name>
  <files>src/reddit_insight/data_source.py</files>
  <action>
  UnifiedDataSource 클래스 추가:

  1. 초기화:
     - __init__(strategy: DataSourceStrategy = DataSourceStrategy.API_FIRST)
     - _api_client: RedditClient | None
     - _scraper: RedditScraper | None
     - _status: SourceStatus
     - _strategy: DataSourceStrategy

  2. 소스 관리:
     - _get_api_client() -> RedditClient
       - lazy initialization
     - _get_scraper() -> RedditScraper
       - lazy initialization
     - get_status() -> SourceStatus

  3. 전환 로직:
     - _should_use_api() -> bool
       - strategy와 status 기반 판단
     - _should_fallback_to_scraping(error: Exception) -> bool
       - rate limit, 인증 실패 등 판별
     - _record_failure(source: str, error: Exception)
     - _record_success(source: str)

  주의:
  - API 인증 정보 없으면 자동으로 스크래핑 사용
  - 연속 실패 시 해당 소스 일시적 비활성화 (5회)
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.data_source import UnifiedDataSource, DataSourceStrategy; ds = UnifiedDataSource(); print('UnifiedDataSource OK')"
  </verify>
  <done>
  - UnifiedDataSource 기본 구조 구현
  - 전환 로직 정의됨
  </done>
</task>

<task type="auto">
  <name>Task 3: 통합 데이터 수집 메서드 구현</name>
  <files>src/reddit_insight/data_source.py</files>
  <action>
  UnifiedDataSource에 데이터 수집 메서드 추가:

  1. 게시물 수집:
     - async get_hot_posts(subreddit: str, limit: int = 100) -> list[Post]
     - async get_new_posts(subreddit: str, limit: int = 100) -> list[Post]
     - async get_top_posts(subreddit: str, time_filter: str = "week", limit: int = 100) -> list[Post]

  2. 댓글 수집:
     - async get_post_comments(post_id: str, limit: int | None = None) -> list[Comment]

  3. 서브레딧 정보:
     - async get_subreddit_info(name: str) -> SubredditInfo | None
     - async search_subreddits(query: str, limit: int = 25) -> list[SubredditInfo]

  4. 내부 헬퍼:
     - async _execute_with_fallback(
         api_func: Callable,
         scraping_func: Callable,
         *args, **kwargs
       ) -> Any
       - strategy에 따라 API/스크래핑 실행
       - 실패 시 폴백
       - 결과 및 실패 기록

  주의:
  - API는 동기, 스크래핑은 비동기
  - API 호출은 asyncio.to_thread()로 래핑
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.data_source import UnifiedDataSource; ds = UnifiedDataSource(); print(dir(ds))"
  </verify>
  <done>
  - 통합 데이터 수집 메서드 구현
  - API/스크래핑 폴백 로직 동작
  </done>
</task>

<task type="auto">
  <name>Task 4: 패키지 통합 및 테스트</name>
  <files>src/reddit_insight/__init__.py, tests/test_data_source.py</files>
  <action>
  1. src/reddit_insight/__init__.py 업데이트:
     - UnifiedDataSource, DataSourceStrategy export
     - __all__ 업데이트

  2. tests/test_data_source.py 생성:
     - TestDataSourceStrategy: 전략 열거형 테스트
     - TestUnifiedDataSource:
       - test_api_first_success: API 성공 시 API 사용
       - test_api_first_fallback: API 실패 시 스크래핑 전환
       - test_scraping_only: 스크래핑만 사용
       - test_failure_tracking: 실패 카운트 추적
     - Mock 사용하여 실제 네트워크 호출 없이 테스트
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight import UnifiedDataSource, DataSourceStrategy; print('Package integration OK')"
  - python -m pytest tests/test_data_source.py -v --collect-only 2>/dev/null || echo "Tests defined"
  </verify>
  <done>
  - 패키지 레벨에서 UnifiedDataSource 사용 가능
  - 기본 테스트 작성됨
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] DataSourceStrategy 열거형 정의됨
- [ ] UnifiedDataSource가 API/스크래핑 전환 지원
- [ ] 패키지 레벨 import 가능
- [ ] 기본 테스트 작성됨
</verification>

<success_criteria>

- 모든 태스크 완료
- API 실패 시 자동으로 스크래핑 폴백
- 사용자는 데이터 소스를 신경 쓰지 않고 데이터 수집 가능
- Phase 3 완료

</success_criteria>

<output>
After completion, create `.planning/phases/03-web-scraping/03-03-SUMMARY.md`
</output>
