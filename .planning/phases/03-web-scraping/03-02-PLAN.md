---
phase: 03-web-scraping
plan: 02
type: execute
depends_on: ["03-01"]
files_modified: [src/reddit_insight/scraping/parser.py, src/reddit_insight/scraping/reddit_scraper.py]
---

<objective>
Reddit 페이지 파서를 구현한다.

Purpose: old.reddit.com의 JSON 및 HTML 응답을 파싱하여 데이터 추출
Output: RedditScraper 클래스, Post/Comment 파싱 기능
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-web-scraping/03-01-PLAN.md

**Research findings:**
- old.reddit.com/{subreddit}.json: 서브레딧 게시물 JSON
- old.reddit.com/comments/{post_id}.json: 게시물 + 댓글 JSON
- JSON 응답은 [listing, listing] 형태 (게시물, 댓글)
- after 파라미터로 페이지네이션

**Prior plan context:**
- ScrapingClient, RateLimiter 구현됨
- Post, Comment 모델은 reddit/models.py에 정의됨 (재사용)

**URL Patterns:**
- https://old.reddit.com/r/{subreddit}/hot.json
- https://old.reddit.com/r/{subreddit}/new.json
- https://old.reddit.com/r/{subreddit}/top.json?t={time}
- https://old.reddit.com/comments/{post_id}.json?limit=500
</context>

<tasks>

<task type="auto">
  <name>Task 1: JSON 응답 파서 구현</name>
  <files>src/reddit_insight/scraping/parser.py</files>
  <action>
  src/reddit_insight/scraping/parser.py 생성:

  1. RedditJSONParser 클래스:
     - parse_listing(data: dict) -> list[dict]
       - Reddit listing 응답에서 children 추출
     - parse_post(data: dict) -> Post
       - JSON 데이터를 Post 모델로 변환
     - parse_comment(data: dict) -> Comment
       - JSON 데이터를 Comment 모델로 변환
     - parse_subreddit(data: dict) -> SubredditInfo
       - JSON 데이터를 SubredditInfo 모델로 변환

  2. 유틸리티 함수:
     - extract_posts_from_response(response: dict) -> list[Post]
     - extract_comments_from_response(response: list) -> list[Comment]
       - 댓글 트리 평탄화 포함

  주의:
  - Reddit JSON 구조: {"kind": "Listing", "data": {"children": [...]}}
  - 각 child: {"kind": "t3", "data": {...}} (t3=post, t1=comment)
  - 삭제된 게시물/댓글 처리 ([deleted], [removed])
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping.parser import RedditJSONParser; print('Parser OK')"
  </verify>
  <done>
  - RedditJSONParser가 JSON 응답 파싱
  - Post, Comment 모델로 변환
  </done>
</task>

<task type="auto">
  <name>Task 2: RedditScraper 기본 구현</name>
  <files>src/reddit_insight/scraping/reddit_scraper.py</files>
  <action>
  src/reddit_insight/scraping/reddit_scraper.py 생성:

  1. RedditScraper 클래스:
     - __init__(client: ScrapingClient | None = None)
     - _client: ScrapingClient
     - _parser: RedditJSONParser
     - BASE_URL = "https://old.reddit.com"

  2. 게시물 수집 메서드:
     - async get_hot(subreddit: str, limit: int = 100) -> list[Post]
     - async get_new(subreddit: str, limit: int = 100) -> list[Post]
     - async get_top(subreddit: str, time_filter: str = "week", limit: int = 100) -> list[Post]
     - async _fetch_posts(url: str, limit: int) -> list[Post]
       - 페이지네이션 처리 (after 파라미터)

  3. URL 생성:
     - _build_subreddit_url(subreddit: str, sort: str, params: dict) -> str

  주의:
  - limit > 100이면 여러 요청으로 페이지네이션
  - 한 요청당 최대 100개
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping.reddit_scraper import RedditScraper; print('RedditScraper OK')"
  </verify>
  <done>
  - RedditScraper가 게시물 수집 지원
  - 페이지네이션 자동 처리
  </done>
</task>

<task type="auto">
  <name>Task 3: 댓글 수집 기능 추가</name>
  <files>src/reddit_insight/scraping/reddit_scraper.py</files>
  <action>
  RedditScraper에 댓글 수집 메서드 추가:

  1. 댓글 수집:
     - async get_post_comments(post_id: str, limit: int = 500) -> list[Comment]
       - /comments/{post_id}.json?limit={limit}
       - 응답 구조: [post_listing, comments_listing]

  2. 서브레딧 댓글 스트림:
     - async get_subreddit_comments(subreddit: str, limit: int = 100) -> list[Comment]
       - /r/{subreddit}/comments.json

  3. 유틸리티:
     - _flatten_comment_tree(comments: list) -> list[Comment]
       - 중첩된 replies를 평탄화
       - "more" 타입 노드 스킵

  주의:
  - 댓글 응답은 중첩 구조
  - kind: "more"는 추가 댓글 존재 표시 (스크래핑에서는 스킵)
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping.reddit_scraper import RedditScraper; s = RedditScraper(); print('Comments methods OK')"
  </verify>
  <done>
  - 댓글 수집 기능 구현
  - 댓글 트리 평탄화 지원
  </done>
</task>

<task type="auto">
  <name>Task 4: 서브레딧 정보 수집 및 export</name>
  <files>src/reddit_insight/scraping/reddit_scraper.py, src/reddit_insight/scraping/__init__.py</files>
  <action>
  1. RedditScraper에 서브레딧 메서드 추가:
     - async get_subreddit_info(name: str) -> SubredditInfo | None
       - /r/{name}/about.json
     - async search_subreddits(query: str, limit: int = 25) -> list[SubredditInfo]
       - /subreddits/search.json?q={query}

  2. __init__.py 업데이트:
     - RedditScraper, RedditJSONParser export
     - __all__ 업데이트
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.scraping import RedditScraper, ScrapingClient, RateLimiter; print('All Phase 3-02 exports OK')"
  </verify>
  <done>
  - 서브레딧 정보 수집 기능 구현
  - 모든 스크래핑 기능 export됨
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] JSON 파서가 Post, Comment, SubredditInfo 변환 지원
- [ ] RedditScraper가 hot/new/top 게시물 수집 가능
- [ ] 댓글 수집 및 트리 평탄화 동작
- [ ] 서브레딧 정보 및 검색 가능
</verification>

<success_criteria>

- 모든 태스크 완료
- RedditScraper가 API와 동일한 데이터 모델 반환
- 페이지네이션 및 댓글 트리 처리 완료

</success_criteria>

<output>
After completion, create `.planning/phases/03-web-scraping/03-02-SUMMARY.md`
</output>
