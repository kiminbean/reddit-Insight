---
phase: 31-final-polish-testing
plan: 01
type: execute
depends_on: ["30-01"]
files_modified: [tests/e2e/test_v2_features.py, tests/performance/test_v2_perf.py, docs/v2-features.md, src/reddit_insight/dashboard/templates/base.html]
---

<objective>
v2.0 최종 폴리싱 및 통합 테스트를 완료한다.

Purpose: v2.0 릴리스 전 품질 보장 - 새 기능 E2E 테스트, 성능 검증, 문서 업데이트
Output: v2.0 기능 E2E 테스트, 성능 테스트, 업데이트된 문서
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**v2.0 새 기능 (테스트 대상):**
- Phase 26-27: LLM 분석 (Claude/OpenAI API)
- Phase 28: 멀티 서브레딧 비교
- Phase 29: 실시간 모니터링 (SSE)
- Phase 30: 알림 시스템 (Email/Webhook)
- Phase 22: 캐싱, 페이지네이션
- Phase 24: PDF/Excel 내보내기

**기존 테스트 현황:**
- tests/llm/ - LLM 단위 테스트
- tests/streaming/ - 모니터 단위 테스트
- tests/alerts/ - 알림 단위 테스트
- tests/analysis/test_comparison.py - 비교 분석 단위 테스트
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create v2.0 E2E Integration Tests</name>
  <files>tests/e2e/test_v2_features.py</files>
  <action>v2.0 신규 기능에 대한 E2E 통합 테스트를 작성한다:

  ```python
  import pytest
  from httpx import AsyncClient
  from unittest.mock import AsyncMock, patch, MagicMock

  class TestLLMIntegration:
      """LLM 분석 통합 테스트"""

      @pytest.mark.asyncio
      async def test_llm_analysis_endpoint(self, client: AsyncClient):
          """LLM 분석 API 엔드포인트 테스트"""
          # Mock LLM client
          with patch("reddit_insight.dashboard.services.llm_service.LLMAnalyzer") as mock:
              mock.return_value.analyze.return_value = {...}
              response = await client.post("/dashboard/llm/analyze", json={"texts": ["test"]})
              assert response.status_code == 200

      @pytest.mark.asyncio
      async def test_llm_page_renders(self, client: AsyncClient):
          """LLM 페이지 렌더링 테스트"""
          response = await client.get("/dashboard/llm")
          assert response.status_code == 200
          assert "LLM" in response.text

  class TestComparisonIntegration:
      """멀티 서브레딧 비교 통합 테스트"""

      @pytest.mark.asyncio
      async def test_comparison_page_renders(self, client: AsyncClient):
          """비교 페이지 렌더링 테스트"""
          response = await client.get("/dashboard/comparison")
          assert response.status_code == 200

      @pytest.mark.asyncio
      async def test_comparison_analyze(self, client: AsyncClient):
          """비교 분석 실행 테스트"""
          with patch("reddit_insight.dashboard.routers.comparison.ComparisonService") as mock:
              mock.return_value.compare_subreddits.return_value = {...}
              response = await client.post(
                  "/dashboard/comparison/analyze",
                  data={"subreddits": ["python", "javascript"]}
              )
              assert response.status_code in [200, 302]

  class TestLiveMonitoringIntegration:
      """실시간 모니터링 통합 테스트"""

      @pytest.mark.asyncio
      async def test_live_page_renders(self, client: AsyncClient):
          """라이브 대시보드 페이지 렌더링 테스트"""
          response = await client.get("/dashboard/live")
          assert response.status_code == 200
          assert "EventSource" in response.text or "live" in response.text.lower()

      @pytest.mark.asyncio
      async def test_sse_stream_endpoint(self, client: AsyncClient):
          """SSE 스트림 엔드포인트 테스트"""
          with patch("reddit_insight.dashboard.routers.live.live_service") as mock:
              mock.start_monitoring = AsyncMock()
              mock.start_monitoring.return_value.subscribe = AsyncMock()
              # SSE는 streaming이므로 연결만 확인
              # 실제 테스트에서는 timeout 설정 필요

  class TestAlertsIntegration:
      """알림 시스템 통합 테스트"""

      @pytest.mark.asyncio
      async def test_alerts_page_renders(self, client: AsyncClient):
          """알림 설정 페이지 렌더링 테스트"""
          response = await client.get("/dashboard/alerts")
          assert response.status_code == 200

      @pytest.mark.asyncio
      async def test_create_alert_rule(self, client: AsyncClient):
          """알림 규칙 생성 테스트"""
          response = await client.post(
              "/dashboard/alerts/rules",
              data={
                  "type": "keyword_surge",
                  "subreddit": "python",
                  "threshold": "50",
                  "notifiers": "webhook"
              }
          )
          assert response.status_code in [200, 201, 302]

      @pytest.mark.asyncio
      async def test_alert_history(self, client: AsyncClient):
          """알림 이력 조회 테스트"""
          response = await client.get("/dashboard/alerts/history")
          assert response.status_code == 200
  ```</action>
  <verify>pytest tests/e2e/test_v2_features.py -v</verify>
  <done>v2.0 E2E 통합 테스트 작성 완료</done>
</task>

<task type="auto">
  <name>Task 2: Create Performance Benchmark Tests</name>
  <files>tests/performance/test_v2_perf.py</files>
  <action>v2.0 기능에 대한 성능 벤치마크 테스트를 작성한다:

  ```python
  import pytest
  import time
  from unittest.mock import patch, MagicMock

  class TestCachePerformance:
      """캐시 성능 테스트"""

      def test_cache_hit_performance(self, cache_service):
          """캐시 히트 시 응답 시간 테스트"""
          # 첫 번째 호출 (캐시 미스)
          cache_service.set("test_key", {"data": "value"}, ttl=60)

          # 캐시 히트 시 응답 시간 측정
          start = time.perf_counter()
          for _ in range(1000):
              cache_service.get("test_key")
          elapsed = time.perf_counter() - start

          # 1000번 조회가 100ms 이내여야 함
          assert elapsed < 0.1, f"Cache hit too slow: {elapsed:.3f}s"

  class TestComparisonPerformance:
      """비교 분석 성능 테스트"""

      def test_comparison_analysis_performance(self):
          """여러 서브레딧 비교 분석 성능 테스트"""
          from reddit_insight.analysis.comparison import ComparisonAnalyzer

          # 5개 서브레딧 비교 시 1초 이내 완료
          # Mock data로 테스트
          start = time.perf_counter()
          # ... 분석 실행
          elapsed = time.perf_counter() - start

          assert elapsed < 1.0, f"Comparison too slow: {elapsed:.3f}s"

  class TestLLMRateLimiterPerformance:
      """Rate Limiter 성능 테스트"""

      def test_rate_limiter_overhead(self):
          """Rate limiter의 오버헤드 테스트"""
          from reddit_insight.llm.rate_limiter import RateLimiter

          limiter = RateLimiter(rpm_limit=60, tpm_limit=100000)

          start = time.perf_counter()
          for _ in range(1000):
              limiter.check_limits(token_count=10)
          elapsed = time.perf_counter() - start

          # 1000번 체크가 50ms 이내여야 함
          assert elapsed < 0.05, f"Rate limiter too slow: {elapsed:.3f}s"

  class TestDashboardResponseTime:
      """대시보드 응답 시간 테스트"""

      @pytest.mark.asyncio
      async def test_dashboard_pages_response_time(self, client):
          """주요 대시보드 페이지 응답 시간 테스트"""
          pages = [
              "/dashboard",
              "/dashboard/trends",
              "/dashboard/demands",
              "/dashboard/competition",
              "/dashboard/insights",
              "/dashboard/llm",
              "/dashboard/comparison",
              "/dashboard/live",
              "/dashboard/alerts",
          ]

          for page in pages:
              start = time.perf_counter()
              response = await client.get(page)
              elapsed = time.perf_counter() - start

              assert response.status_code == 200, f"{page} failed"
              assert elapsed < 0.5, f"{page} too slow: {elapsed:.3f}s"
  ```</action>
  <verify>pytest tests/performance/test_v2_perf.py -v --tb=short</verify>
  <done>v2.0 성능 벤치마크 테스트 작성 완료</done>
</task>

<task type="auto">
  <name>Task 3: Update Documentation for v2.0 Features</name>
  <files>docs/v2-features.md, docs/user-guide.md</files>
  <action>v2.0 신규 기능에 대한 문서를 추가/업데이트한다:

  1. docs/v2-features.md 생성:
     - LLM 분석 기능 사용법
     - 멀티 서브레딧 비교 기능
     - 실시간 모니터링 설정
     - 알림 시스템 구성
     - PDF/Excel 내보내기

  2. docs/user-guide.md 업데이트:
     - v2.0 기능 섹션 추가
     - 설정 가이드 업데이트 (LLM API 키, SMTP 설정 등)

  3. README.md 업데이트:
     - v2.0 기능 하이라이트 추가
     - 빠른 시작 가이드 업데이트</action>
  <verify>cat docs/v2-features.md | head -50</verify>
  <done>v2.0 문서 업데이트 완료</done>
</task>

<task type="auto">
  <name>Task 4: Final Verification and Cleanup</name>
  <files>src/reddit_insight/dashboard/templates/base.html</files>
  <action>최종 검증 및 정리 작업을 수행한다:

  1. 전체 테스트 스위트 실행:
     ```bash
     pytest tests/ -v --tb=short
     ```

  2. 타입 체크 (mypy):
     ```bash
     mypy src/reddit_insight --ignore-missing-imports
     ```

  3. 린트 체크 (ruff):
     ```bash
     ruff check src/reddit_insight
     ```

  4. 대시보드 네비게이션 최종 확인:
     - 모든 v2.0 기능 메뉴가 표시되는지 확인
     - LLM, Comparison, Live, Alerts 메뉴 존재 확인

  5. 서버 시작 및 수동 확인:
     ```bash
     PYTHONPATH=src .venv/bin/uvicorn reddit_insight.dashboard.app:app --port 8888
     ```</action>
  <verify>pytest tests/ -v --tb=short -q | tail -20</verify>
  <done>최종 검증 및 정리 완료</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] v2.0 E2E 테스트 통과
- [ ] 성능 벤치마크 테스트 통과
- [ ] docs/v2-features.md 생성됨
- [ ] 전체 테스트 스위트 통과
- [ ] 대시보드 모든 기능 접근 가능
</verification>

<success_criteria>

- v2.0 기능 E2E 통합 테스트 작성 및 통과
- 성능 벤치마크 테스트 작성 및 통과
- v2.0 기능 문서 완성
- 전체 테스트 스위트 통과 (500+ tests)
- 대시보드 모든 페이지 정상 작동
</success_criteria>

<output>
After completion, create `.planning/phases/31-final-polish-testing/31-01-SUMMARY.md`
</output>
