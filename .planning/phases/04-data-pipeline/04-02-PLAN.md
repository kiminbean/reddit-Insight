---
phase: 04-data-pipeline
plan: 02
type: execute
depends_on: ["04-01"]
files_modified: [src/reddit_insight/storage/repository.py, src/reddit_insight/pipeline/__init__.py, src/reddit_insight/pipeline/preprocessor.py]
---

<objective>
데이터 전처리 파이프라인을 구축한다.

Purpose: 수집된 Reddit 데이터를 정제하고 분석 가능한 형태로 변환
Output: Repository 패턴, 텍스트 전처리기, 데이터 정규화
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-data-pipeline/04-01-PLAN.md

**Prior plan context:**
- Database, PostModel, CommentModel, SubredditModel 정의됨
- Pydantic ↔ ORM 변환 지원

**Preprocessing needs:**
- 텍스트 정규화 (URL 제거, 특수문자 처리)
- 삭제된 콘텐츠 필터링 ([deleted], [removed])
- 중복 제거
- 타임스탬프 정규화
</context>

<tasks>

<task type="auto">
  <name>Task 1: Repository 패턴 구현</name>
  <files>src/reddit_insight/storage/repository.py</files>
  <action>
  src/reddit_insight/storage/repository.py 생성:

  1. BaseRepository 클래스:
     - __init__(session: AsyncSession)
     - _session: AsyncSession

  2. SubredditRepository:
     - async get_by_name(name: str) -> SubredditModel | None
     - async get_or_create(info: SubredditInfo) -> SubredditModel
     - async update_metrics(name: str, subscribers: int, ...)
     - async list_all(limit: int = 100) -> list[SubredditModel]

  3. PostRepository:
     - async get_by_reddit_id(reddit_id: str) -> PostModel | None
     - async save(post: Post, subreddit_id: int) -> PostModel
     - async save_many(posts: list[Post], subreddit_id: int) -> list[PostModel]
     - async get_by_subreddit(subreddit_id: int, limit: int = 100) -> list[PostModel]
     - async get_recent(hours: int = 24, limit: int = 100) -> list[PostModel]

  4. CommentRepository:
     - async get_by_reddit_id(reddit_id: str) -> CommentModel | None
     - async save(comment: Comment, post_id: int) -> CommentModel
     - async save_many(comments: list[Comment], post_id: int) -> list[CommentModel]
     - async get_by_post(post_id: int) -> list[CommentModel]

  주의:
  - save_many는 bulk insert 사용
  - 중복 처리: ON CONFLICT 또는 get_or_create 패턴
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.storage.repository import PostRepository, CommentRepository, SubredditRepository; print('Repositories OK')"
  </verify>
  <done>
  - Repository 패턴으로 데이터 접근 추상화
  - CRUD 및 bulk 연산 지원
  </done>
</task>

<task type="auto">
  <name>Task 2: 텍스트 전처리기 구현</name>
  <files>src/reddit_insight/pipeline/__init__.py, src/reddit_insight/pipeline/preprocessor.py</files>
  <action>
  1. src/reddit_insight/pipeline/ 디렉토리 생성

  2. src/reddit_insight/pipeline/preprocessor.py 생성:

  TextPreprocessor 클래스:
  - clean_text(text: str) -> str:
    - URL 제거 (http/https)
    - Reddit 멘션 정규화 (/u/username, /r/subreddit)
    - HTML 엔티티 디코딩 (&amp;, &lt; 등)
    - 연속 공백 정규화
    - 줄바꿈 정규화

  - is_deleted_content(text: str) -> bool:
    - "[deleted]", "[removed]" 체크

  - normalize_author(author: str) -> str | None:
    - "[deleted]" -> None
    - 정상 username 반환

  - extract_urls(text: str) -> list[str]:
    - 텍스트에서 URL 추출

  - extract_mentions(text: str) -> dict:
    - users: list[str] (/u/ 멘션)
    - subreddits: list[str] (/r/ 멘션)

  3. __init__.py 생성:
     - TextPreprocessor export
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.pipeline import TextPreprocessor; p = TextPreprocessor(); print(p.clean_text('Hello &amp; World!'))"
  </verify>
  <done>
  - 텍스트 정규화 및 정제 기능
  - 삭제 콘텐츠 필터링
  </done>
</task>

<task type="auto">
  <name>Task 3: 데이터 파이프라인 클래스</name>
  <files>src/reddit_insight/pipeline/data_pipeline.py</files>
  <action>
  src/reddit_insight/pipeline/data_pipeline.py 생성:

  1. DataPipeline 클래스:
     - __init__(database: Database)
     - _db: Database
     - _preprocessor: TextPreprocessor

  2. 수집 → 저장 파이프라인:
     - async process_posts(posts: list[Post], subreddit_name: str) -> ProcessingResult
       - 중복 필터링
       - 텍스트 전처리
       - 데이터베이스 저장
       - 결과 통계 반환

     - async process_comments(comments: list[Comment], post_reddit_id: str) -> ProcessingResult
       - 삭제 댓글 필터링
       - 텍스트 전처리
       - 데이터베이스 저장

  3. ProcessingResult 데이터 클래스:
     - total: int
     - new: int
     - duplicates: int
     - filtered: int (삭제된 콘텐츠)
     - errors: int

  4. 전체 수집 워크플로우:
     - async collect_and_store(
         subreddit: str,
         sort: str = "hot",
         limit: int = 100,
         include_comments: bool = False
       ) -> dict
       - UnifiedDataSource로 수집
       - 파이프라인으로 처리
       - 결과 요약 반환
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.pipeline.data_pipeline import DataPipeline, ProcessingResult; print('DataPipeline OK')"
  </verify>
  <done>
  - 수집 → 전처리 → 저장 파이프라인 완성
  - 처리 결과 통계 제공
  </done>
</task>

<task type="auto">
  <name>Task 4: export 및 통합</name>
  <files>src/reddit_insight/pipeline/__init__.py, src/reddit_insight/storage/__init__.py</files>
  <action>
  1. pipeline/__init__.py 업데이트:
     - TextPreprocessor, DataPipeline, ProcessingResult export
     - __all__ 정의

  2. storage/__init__.py 업데이트:
     - PostRepository, CommentRepository, SubredditRepository export
     - __all__ 업데이트
  </action>
  <verify>
  - python -c "import sys; sys.path.insert(0, 'src'); from reddit_insight.pipeline import DataPipeline, TextPreprocessor; from reddit_insight.storage import PostRepository; print('All exports OK')"
  </verify>
  <done>
  - 모든 모듈 export 완료
  - 파이프라인 사용 준비됨
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Repository 패턴으로 데이터 접근 가능
- [ ] TextPreprocessor가 텍스트 정제
- [ ] DataPipeline이 수집→저장 워크플로우 제공
- [ ] 모든 모듈 import 가능
</verification>

<success_criteria>

- 모든 태스크 완료
- 데이터 파이프라인이 end-to-end 동작
- 중복 및 삭제 콘텐츠 필터링 동작

</success_criteria>

<output>
After completion, create `.planning/phases/04-data-pipeline/04-02-SUMMARY.md`
</output>
